meta:
  dataset: eu-emissions-trading-system
  findability: public
  # username and userid could be found in ~/.config/datahub/config.json
  owner: core
  ownerid: a08d3588fbae0355042537595c65819d

inputs:
  -
    kind: datapackage
    # the datapackage.json url. Soon this file will be grabbed automatically
    url: https://raw.githubusercontent.com/datasets/<dataset>/master/.datahub/datapackage.json
    parameters:
      descriptor:
        # this name will be used in the next steps
        name: data-source
        title: Title
        homepage: http://source-site.com/
        version: 0.0.1
        license: <MIT, GPL, etc?>
        # this section should match the source data structure
        # use datapackage-py infer method to get this in json format
        resources:
          -
            name: data-source (not sure we need this)
            path: localpath to csv file (not sure we need this)
            format: csv
            mediatype: text/csv
            "schema":
              "fields":
                -
                  "name": "id"
                  "type": "integer"
                -
                  "name": "type"
                  "type": "string"
                -
                  "name": "name"
                  "type": "string"
                # etc


      resource-mapping:
        # the link to original data-source file
        data-source: http://source-site.com/datafile.csv

# this part describes what to do with data, how to 'process' it
processing:
  -
    input: data-source
    # there should be output resource,
    # which you should describe in the proper section
    # but for now place the same name
    output: data-source


    # processors - the programs that will be called to wrangle your data. see:
    # https://github.com/frictionlessdata/datapackage-pipelines - dpp
    # https://github.com/frictionlessdata/tabulator-py - tabulator
    # may be more, @irakly plz add links here
    dpp:
      # see datapackage-pipelines docs how to use this section:
      -
        run: dpp_task
        parameters:
          resources: data-source
          fields:
            -
              # fill this
      -
        run: delete_fields  # this is for example
        parameters:
          resources: data-source
          fields:
            - id
            - home_link
            - keywords


# how often to run the automation?
schedule: 'every 1d'